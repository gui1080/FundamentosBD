{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTbJkCSE7-Nm"
   },
   "source": [
    "## Universidade de Brasília\n",
    "## Instituto de Ciências Exatas\n",
    "## Departamento de Ciência da Computação - PPCA\n",
    "## Disciplina: Fundamentos de Banco de Dados\n",
    "Projeto de Banco de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8Z9u_8lW778X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importa módulos usados\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import types\n",
    "import sqlalchemy\n",
    "import psycopg2\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Definindo funções\n",
    "def ler_csv(arquivo, separador, colunas, codificacao, nomes_colunas):\n",
    "    df = pd.read_csv(arquivo, delimiter=separador, usecols=colunas, encoding=codificacao)\n",
    "    df.columns = nomes_colunas\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PjSHkd5PKQNV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "# cwd = os.getcwd()\n",
    "# print(\"Current working directory: {0}\".format(cwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWM554VWA9oU"
   },
   "source": [
    "## Órgãos do SIAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "RPXo4u_vDn9D",
    "outputId": "fb390a96-b868-49c5-fb7b-c2a015ae7f8f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv = 'orgaos.CSV'\n",
    "df_orgao = pd.read_csv(csv, encoding='ISO-8859-1')\n",
    "df_orgao.columns = ['cod', 'nome', 'cnpj', 'codpoder', 'nomepoder', 'codtipoadministracao', 'nometipoadministracao']\n",
    "print('{:,}'.format(len(df_orgao)) + \" rows\")\n",
    "\n",
    "# Criando dataframe que vira tabela \"poder\"\n",
    "df_poder = df_orgao[['codpoder', 'nomepoder']].copy()\n",
    "df_orgao.drop(columns=['nomepoder'])\n",
    "\n",
    "# Criando dataframe que vira tabela \"tipo de administração\"\n",
    "df_tipo_administracao = df_orgao[['codtipoadministracao', 'nometipoadministracao']].copy()\n",
    "df_orgao.drop(columns=['nometipoadministracao'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Viagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [0, 1, 2, 3, 4, 5, 7, 9, 10, 14, 15, 16, 17, 18, 19]\n",
    "campos = [ 'idprocessoviagem', 'numproposta', 'situacao', 'viagemurgente', 'justificativaurgencia',\n",
    "           'codorgsuperior', 'codorgpagador', 'cpfviajante', 'nome', 'datainicio', \n",
    "           'datafim', 'destinos', 'motivo', 'valordiarias', 'valorpassagens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023, 2022, 2021, 2020, 2019\n",
    "df_viagem = ler_csv('2023_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df = ler_csv('2022_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_viagem = pd.concat([df_viagem, df])\n",
    "df = ler_csv('2021_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_viagem = pd.concat([df_viagem, df])\n",
    "df = ler_csv('2020_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_viagem = pd.concat([df_viagem, df])\n",
    "df = ler_csv('2019_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_viagem = pd.concat([df_viagem, df])\n",
    "print('{:,}'.format(len(df_viagem)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos dados de Viagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sim e não para boolean\n",
    "df_viagem['viagemurgente'] = df_viagem['viagemurgente'].map({'sim': True, 'não': False})\n",
    "df_viagem['viagemurgente'].astype(bool)\n",
    "# Troca vírgula por ponto no float\n",
    "# Convertendo a coluna valor para float\n",
    "df_viagem['valordiarias'] = df_viagem['valordiarias'].str.replace(',', '.')\n",
    "df_viagem['valordiarias'] = df_viagem['valordiarias'].astype(float)\n",
    "df_viagem['valorpassagens'] = df_viagem['valorpassagens'].str.replace(',', '.')\n",
    "df_viagem['valorpassagens'] = df_viagem['valorpassagens'].astype(float)\n",
    "# Unificando código do ministério do planejamento\n",
    "df_viagem['codorgsuperior'] = df_viagem['codorgsuperior'].replace(47000, 20113)\n",
    "# Par \"cpf\" e \"nome\" representa atributo identificador e não pode estar vazio\n",
    "df_viagem['cpfviajante'].fillna(\"Não informado\", inplace = True)\n",
    "df_viagem['nome'].fillna(\"Não informado\", inplace = True)\n",
    "# Dropando linhas duplicadas\n",
    "df_viagem = df_viagem.drop_duplicates(subset=['idprocessoviagem'])\n",
    "print('{:,}'.format(len(df_viagem)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pessoas\n",
    "\n",
    "Originário da tabela viagem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [ 9, 10, 11, 12, 13]\n",
    "campos = [ 'cpfviajante', 'nome', 'cargo', 'funcao', 'descricaofuncao' ]\n",
    "\n",
    "# 2023, 2022, 2021, 2020, 2019\n",
    "df_passageiros = ler_csv('2023_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df = ler_csv('2022_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_passageiros = pd.concat([df_passageiros, df])\n",
    "df = ler_csv('2021_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_passageiros = pd.concat([df_passageiros, df])\n",
    "df = ler_csv('2020_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_passageiros = pd.concat([df_passageiros, df])\n",
    "df = ler_csv('2019_Viagem.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_passageiros = pd.concat([df_passageiros, df])\n",
    "print('{:,}'.format(len(df_passageiros)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza de dados de Pessoas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Par \"cpf\" e \"nome\" representa atributo identificador e não pode estar vazio\n",
    "df_passageiros['cpfviajante'].fillna(\"Não informado\", inplace = True)\n",
    "df_passageiros['nome'].fillna(\"Não informado\", inplace = True)\n",
    "\n",
    "df_passageiros['cargo'].fillna(\"Desconhecido\", inplace = True)\n",
    "df_passageiros['funcao'].fillna(\"Desconhecido\", inplace = True)\n",
    "df_passageiros['descricaofuncao'].fillna(\"Desconhecido\", inplace = True)\n",
    "\n",
    "# remover duplicatas\n",
    "df_passageiros=df_passageiros.drop_duplicates(subset=['cpfviajante', 'nome'], keep='first')\n",
    "print('{:,}'.format(len(df_passageiros)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trechos de Viagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols =   [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "campos = ['idprocessoviagem', 'seqtrecho', 'dataorigem', 'paisorigem', 'uforigem', 'cidadeorigem',\n",
    "          'datadestino', 'paisdestino', 'ufdestino', 'cidadedestino',\n",
    "          'meiotrasnporte', 'numdiarias', 'missao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023, 2022, 2021, 2020, 2019\n",
    "df_trecho = ler_csv('2023_Trecho.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df = ler_csv('2022_Trecho.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_trecho = pd.concat([df_trecho, df])\n",
    "df = ler_csv('2021_Trecho.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_trecho = pd.concat([df_trecho, df])\n",
    "df = ler_csv('2020_Trecho.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_trecho = pd.concat([df_trecho, df])\n",
    "df = ler_csv('2019_Trecho.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_trecho = pd.concat([df_trecho, df])\n",
    "print('{:,}'.format(len(df_trecho)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGEM\n",
    "\n",
    "df_locais_origem = df_trecho[['paisorigem', 'uforigem', 'cidadeorigem']].copy()\n",
    "\n",
    "df_locais_origem.sort_values(by=['cidadeorigem'], inplace=True)\n",
    "\n",
    "# remove cidades duplicadas (nacionais e internacionais)\n",
    "df_locais_origem = df_locais_origem.drop_duplicates(subset='cidadeorigem', keep=\"first\")\n",
    "\n",
    "df_cidades = df_locais_origem[['cidadeorigem', 'paisorigem', 'uforigem']].copy()\n",
    "df_cidades['id'] = df_cidades.index\n",
    "\n",
    "# coluna internacional\n",
    "df_cidades['dest_internacional'] = \"\"\n",
    "\n",
    "# coluna destino nacional\n",
    "df_cidades['dest_internacional'] = \"\"\n",
    "\n",
    "df_cidades = df_cidades.astype(str)\n",
    "\n",
    "df_ufs = df_locais_origem[['uforigem', 'paisorigem']].copy()\n",
    "df_ufs = df_ufs.drop_duplicates(subset='uforigem', keep=\"first\")\n",
    "df_ufs['id'] = df_ufs.index\n",
    "\n",
    "# coluna pais da uf\n",
    "df_ufs[\"pais\"] = \"\"\n",
    "\n",
    "df_ufs = df_ufs.astype(str)\n",
    "\n",
    "df_paises = df_locais_origem[['paisorigem']].copy()\n",
    "df_paises = df_paises.drop_duplicates(subset='paisorigem', keep=\"first\")\n",
    "df_paises['id'] = df_paises.index\n",
    "\n",
    "df_paises = df_cidades.astype(str)\n",
    "\n",
    "# preenche as colunas CIDADES\n",
    "for index, row in df_cidades.iterrows():\n",
    "\n",
    "    # internacional\n",
    "    if str(row['uforigem']) == \"\":\n",
    "\n",
    "        row['dest_internacional'] = str(df_paises[df_paises[\"paisorigem\"] == str(row['paisorigem'])]['id'].values[0])\n",
    "\n",
    "    # nacional\n",
    "    else:\n",
    "        row['dest_nacional'] = str(df_ufs[df_ufs[\"uforigem\"] == str(row['uforigem'])]['id'].values[0])\n",
    "\n",
    "# preenche as colunas UFs\n",
    "for index, row in df_ufs.iterrows():\n",
    "\n",
    "    row['pais'] = str(df_paises[df_paises[\"paisorigem\"] == str(row['paisorigem'])]['id'].values[0])\n",
    "\n",
    "\n",
    "# drop nas colunas sem uso nas tabelas\n",
    "\n",
    "df_cidades = df_cidades.drop('paisorigem', axis=1)\n",
    "df_cidades = df_cidades.drop('uforigem', axis=1)\n",
    "\n",
    "df_ufs = df_ufs.drop('paisorigem', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESTINO\n",
    "\n",
    "df_locais_destino = df_trecho[['paisdestino', 'ufdestino', 'cidadedestino']].copy()\n",
    "\n",
    "df_locais_destino.sort_values(by=['cidadedestino'], inplace=True)\n",
    "\n",
    "# remove cidades duplicadas (nacionais e internacionais)\n",
    "df_locais_destino = df_locais_destino.drop_duplicates(subset='cidadedestino', keep=\"first\")\n",
    "\n",
    "df_cidades_append = df_locais_destino[['paisdestino', 'ufdestino', 'cidadedestino']].copy()\n",
    "df_cidades_append['id'] = df_cidades_append.index\n",
    "\n",
    "# coluna internacional\n",
    "df_cidades_append['dest_internacional'] = \"\"\n",
    "\n",
    "# coluna destino nacional\n",
    "df_cidades_append['dest_internacional'] = \"\"\n",
    "\n",
    "df_cidades_append = df_cidades_append.astype(str)\n",
    "\n",
    "df_ufs_append = df_locais_destino[['ufdestino', 'paisdestino']].copy()\n",
    "df_ufs_append = df_ufs_append.drop_duplicates(subset='ufdestino', keep=\"first\")\n",
    "df_ufs_append['id'] = df_ufs_append.index\n",
    "\n",
    "# coluna pais da uf\n",
    "df_ufs_append[\"pais\"] = \"\"\n",
    "\n",
    "df_ufs_append = df_ufs_append.astype(str)\n",
    "\n",
    "df_paises_append = df_locais_destino[['paisdestino']].copy()\n",
    "df_paises_append = df_paises_append.drop_duplicates(subset='paisdestino', keep=\"first\")\n",
    "df_paises_append['id'] = df_paises_append.index\n",
    "\n",
    "df_paises_append = df_paises_append.astype(str)\n",
    "\n",
    "# preenche as colunas CIDADES\n",
    "for index, row in df_cidades_append.iterrows():\n",
    "\n",
    "    # internacional\n",
    "    if str(row['ufdestino']) == \"\":\n",
    "\n",
    "        row['dest_internacional'] = str(df_paises_append[df_paises_append[\"paisdestino\"] == str(row['paisdestino'])]['id'].values[0])\n",
    "\n",
    "    # nacional\n",
    "    else:\n",
    "        row['dest_nacional'] = str(df_ufs_append[df_ufs_append[\"ufdestino\"] == str(row['ufdestino'])]['id'].values[0])\n",
    "\n",
    "# preenche as colunas UFs\n",
    "for index, row in df_ufs_append.iterrows():\n",
    "\n",
    "    row['pais'] = str(df_paises_append[df_paises_append[\"paisdestino\"] == str(row['paisdestino'])]['id'].values[0])\n",
    "\n",
    "\n",
    "# drop nas colunas sem uso nas tabelas\n",
    "\n",
    "df_cidades_append = df_cidades_append.drop('paisdestino', axis=1)\n",
    "df_cidades_append = df_cidades_append.drop('ufdestino', axis=1)\n",
    "\n",
    "df_ufs_append = df_ufs_append.drop('paisdestino', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atualiza os campos de origens em df_trechos\n",
    "\n",
    "df_trecho.drop(columns=['paisorigem', 'uforigem', 'paisdestino', 'paisorigem'])\n",
    "\n",
    "df_cidades.rename(columns = {'cidadeorigem': 'cidade', 'paisorigem': 'pais', 'uforigem': 'uf'}, inplace = True)\n",
    "df_cidades_append.rename(columns = {'paisdestino': 'pais', 'ufdestino': 'uf', 'cidadedestino': 'cidade'}, inplace = True)\n",
    "df_cidades.append(df_cidades_append)\n",
    "df_cidades = df_cidades.drop_duplicates(subset='cidade', keep=\"first\")\n",
    "\n",
    "df_ufs.rename(columns = {'uforigem': 'uf', 'paisorigem': 'pais'}, inplace = True)\n",
    "df_ufs_append.rename(columns = {'ufdestino': 'uf', 'paisdestino': 'pais'}, inplace = True)\n",
    "df_ufs.append(df_ufs_append)\n",
    "df_ufs = df_ufs.drop_duplicates(subset='uf', keep=\"first\")\n",
    "\n",
    "df_paises.rename(columns = {'paisorigem':'pais'}, inplace = True)\n",
    "df_paises_append.rename(columns = {'paisdestino':'pais'}, inplace = True)\n",
    "df_paises.append(df_paises_append)\n",
    "df_paises = df_paises.drop_duplicates(subset='pais', keep=\"first\")\n",
    "\n",
    "# agora cidade origem / cidade destino é relacionamento com codigo de cidade na tabela cidade\n",
    "\n",
    "for index, row in df_trecho.iterrows():\n",
    "    \n",
    "    cidade = str(row['cidadeorigem'])\n",
    "    row['cidadeorigem'] = str(df_cidades[df_cidades[\"cidade\"] == cidade]['id'].values[0])\n",
    "    \n",
    "    \n",
    "    cidade = str(row['cidadedestino'])\n",
    "    row['cidadedestino'] = str(df_cidades[df_cidades[\"cidade\"] == cidade]['id'].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpando os dados de Trechos de Viagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sim e não para boolean\n",
    "df_trecho['missao'] = df_trecho['missao'].map({'sim': True, 'não': False})\n",
    "df_trecho['missao'].astype(bool)\n",
    "# Convertendo a coluna valor para float\n",
    "df_trecho['numdiarias'] = df_trecho['numdiarias'].str.replace(',', '.')\n",
    "df_trecho['numdiarias'] = df_trecho['numdiarias'].astype(float)\n",
    "# Verificando se há alguma linha com idviagem sem pai\n",
    "df_trecho = df_trecho[df_trecho['idprocessoviagem'].isin(df_viagem['idprocessoviagem'])]\n",
    "# Dropando linhas duplicadas\n",
    "df_trecho = df_trecho.drop_duplicates(subset=['idprocessoviagem', 'seqtrecho'])\n",
    "print('{:,}'.format(len(df_trecho)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxXy90f-SQbN"
   },
   "source": [
    "## Pagamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Kj-18QNZu3oQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = [0, 1, 2, 4, 6, 8, 9]\n",
    "campos = ['idprocessoviagem', 'numproposta', 'codorgsuperior', 'codorgpagador', 'codunidgestorapagadora', 'tipopagamento', 'valor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Huu8uu_CDESt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2023, 2022, 2021, 2020, 2019\n",
    "df_pagamento = ler_csv('2023_Pagamento.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df = ler_csv('2022_Pagamento.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_pagamento = pd.concat([df_pagamento, df])\n",
    "df = ler_csv('2021_Pagamento.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_pagamento = pd.concat([df_pagamento, df])\n",
    "df = ler_csv('2020_Pagamento.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_pagamento = pd.concat([df_pagamento, df])\n",
    "df = ler_csv('2019_Pagamento.csv', ';', cols, 'ISO-8859-1', campos)\n",
    "df_pagamento = pd.concat([df_pagamento, df])\n",
    "print ('{:,}'.format(len(df_pagamento)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDqlzRP2gLvO"
   },
   "source": [
    "### Limpando os dados de Pagamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oMMHXhyHgLWe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertendo a coluna valor para float\n",
    "df_pagamento['valor'] = df_pagamento['valor'].str.replace(',', '.')\n",
    "df_pagamento['valor'] = df_pagamento['valor'].astype(float)\n",
    "# Acertando código de orgão sigiloso\n",
    "df_pagamento.loc[df_pagamento['codorgsuperior'] <= 0, 'codorgsuperior'] = -1\n",
    "df_pagamento.loc[df_pagamento['codorgpagador'] <= 0, 'codorgpagador'] = -1\n",
    "df_pagamento.loc[df_pagamento['codunidgestorapagadora'] <= 0, 'codunidgestorapagadora'] = -1\n",
    "# Unificando código do ministério do planejamento\n",
    "df_pagamento['codorgsuperior'] = df_pagamento['codorgsuperior'].replace(47000, 20113)\n",
    "# Verificando se há alguma linha com idviagem sem pai\n",
    "df_pagamento = df_pagamento[df_pagamento['idprocessoviagem'].isin(df_viagem['idprocessoviagem'])]\n",
    "print ('{:,}'.format(len(df_pagamento)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7tsjI7138Jm",
    "outputId": "b81e5d1b-c150-4d7a-f66c-44d620c712fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "soma = df_pagamento['valor'].sum()\n",
    "print('Valor total    : ' + 'R$ {:,.2f}'.format(soma).replace(\",\", \";\").replace(\".\", \",\").replace(\";\", \".\"))\n",
    "soma = df_pagamento.loc[df_pagamento['codunidgestorapagadora'] <= 0, 'valor'].sum()\n",
    "print('Valor em sigilo: ' + 'R$ {:,.2f}'.format(soma).replace(\",\", \";\").replace(\".\", \",\").replace(\";\", \".\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DywqcEbpuU-T",
    "tags": []
   },
   "source": [
    "## Conexão com o Banco de dados e Carga dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inicio_transacao = datetime.datetime.now()\n",
    "\n",
    "###########\n",
    "engine = create_engine('postgresql://postgres:postgres@172.22.22.231:5432/fbdprojeto')\n",
    "df_paises.to_sql('pais', engine, if_exists='append', index=False)\n",
    "df_ufs_append.to_sql('uf', engine, if_exists='append', index=False)\n",
    "df_cidades.to_sql('cidade', engine, if_exists='append', index=False)\n",
    "df_poder.to_sql('poder', engine, if_exists='append', index=False)\n",
    "df_tipo_administracao.to_sql('tipo_administracao', engine, if_exists='append', index=False)\n",
    "df_orgao.to_sql('orgao', engine, if_exists='append', index=False)\n",
    "df_passageiros.to_sql('passageiro', engine, if_exists='append', index=False)\n",
    "df_viagem.to_sql('viagem', engine, if_exists='append', index=False)\n",
    "df_pagamento.to_sql('pagamento', engine, if_exists='append', index=False)\n",
    "df_trecho.to_sql('trecho', engine, if_exists='append', index=False)\n",
    "###########\n",
    "\n",
    "fim_transacao = datetime.datetime.now()\n",
    "print('Tempo total de carga: {}'.format(fim_transacao - inicio_transacao))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
